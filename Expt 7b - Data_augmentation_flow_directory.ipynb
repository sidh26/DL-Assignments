{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data-augmentation_flow_directory.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidh26/DL-Assignments/blob/master/Expt%207b%20-%20Data_augmentation_flow_directory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EKwXmlxj0WY",
        "colab_type": "text"
      },
      "source": [
        "#How to get Images from ImageNet with Python in Google Colaboratory\n",
        "\n",
        "The first step to train a model for image recognition is finding images that belong to the desired class (or classes), and ImageNet is very useful for this because it currently has 14,197,122 images with 21841 synsets indexed. ImageNet aims to provide on average 1000 images to illustrate each one of their 100,000 synsets, the majority of the synsets are nouns (80.000+).\n",
        "\n",
        "For instance if the synset needed is pictures of ships it can be found by searching for ship on the imagenet website and the result will be the following page which has the wnid: n04194289"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz42zfTtkKKQ",
        "colab_type": "text"
      },
      "source": [
        "#Get the list of URLs for the images of the synset:\n",
        "\n",
        "Said list of URLs can be downloaded from the URL http://www.image-net.org/api/text/imagenet.synset.geturls?wnid= followed by the wnid so in the case of ships it would be â€œhttp://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n04194289\" this can be done with the Python library BeautifulSoup:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm2HdBQgjkY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code part 1\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import requests\n",
        "import cv2\n",
        "import PIL.Image\n",
        "import urllib\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "t_page = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n01871265\")#tusker synset\n",
        "# print(t_page.content)\n",
        "\n",
        "# BeautifulSoup is an HTML parsing library\n",
        "\n",
        "t_soup = BeautifulSoup(t_page.content, 'html.parser')#puts the content of the website into the soup variable, each url on a different line\n",
        "#print(soup)\n",
        "#print(soup.prettify())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrI-fU3HkRPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code part 1.1\n",
        "ae_page = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02504458\")#african elephant synset\n",
        "# print(ae_page.content)\n",
        "\n",
        "# BeautifulSoup is an HTML parsing library\n",
        "from bs4 import BeautifulSoup\n",
        "ae_soup = BeautifulSoup(ae_page.content, 'html.parser')#puts the content of the website into the soup variable, each url on a different line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuWme0REcwlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ie_page = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02504013\")#indian elephant synset\n",
        "# print(ie_page.content)\n",
        "\n",
        "# BeautifulSoup is an HTML parsing library\n",
        "from bs4 import BeautifulSoup\n",
        "ie_soup = BeautifulSoup(ie_page.content, 'html.parser')#puts the content of the website into the soup variable, each url on a different line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvp44JU0kWzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "eefc45e5-1060-477f-a8c3-49353c233764"
      },
      "source": [
        "#code part 2\n",
        "t_str_soup=str(t_soup)#convert soup to string so it can be split\n",
        "type(t_str_soup)\n",
        "t_split_urls=t_str_soup.split('\\r\\n')#split so each url is a different possition on a list\n",
        "print(len(t_split_urls))#print the length of the list so you know how many urls you have\n",
        "\n",
        "#code part 2.2\n",
        "ae_str_soup=str(ae_soup)#convert soup to string so it can be split\n",
        "type(ae_str_soup)\n",
        "ae_split_urls=ae_str_soup.split('\\r\\n')#split so each url is a different possition on a list\n",
        "print(len(ae_split_urls))\n",
        "\n",
        "#code part 2.3\n",
        "ie_str_soup=str(ie_soup)#convert soup to string so it can be split\n",
        "type(ie_str_soup)\n",
        "ie_split_urls=ie_str_soup.split('\\r\\n')#split so each url is a different possition on a list\n",
        "print(len(ie_split_urls))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1065\n",
            "2278\n",
            "1650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-8ZThdTkaK7",
        "colab_type": "text"
      },
      "source": [
        "#Make folders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXg_vOrmt_Mg",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://upscfever.com/datasets/flow_from_directory.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvnNPUMdkciv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code part 3\n",
        "#check if all the images where stored on the files system\n",
        "!mkdir /content/train #create the Train folder\n",
        "!mkdir /content/train/tusker #create the tuskers folder\n",
        "!mkdir /content/train/african #create the african elephants folder\n",
        "!mkdir /content/train/indian #create the indian elephants folder\n",
        "!mkdir /content/validation\n",
        "!mkdir /content/validation/tusker #create the tuskers folder\n",
        "!mkdir /content/validation/african #create the african elephants folder\n",
        "!mkdir /content/validation/indian #create the indian elephants folder\n",
        "#!ls /content/train/ships #list the files inside ships\n",
        "!mkdir /content/test/ \n",
        "!mkdir /content/test/test #list the files inside test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws5t46y-kg6X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34aa0316-029b-461f-884f-05352c7f73d8"
      },
      "source": [
        "#code part 4\n",
        "img_rows, img_cols = 32, 32 #number of rows and columns to convert the images to\n",
        "input_shape = (img_rows, img_cols, 3)#format to store the images (rows, columns,channels) called channels last\n",
        "\n",
        "def url_to_image(url):\n",
        "\t# download the image, convert it to a NumPy array, and then read\n",
        "\t# it into OpenCV format\n",
        "\tresp = urllib.request.urlopen(url)\n",
        "\timage = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "\timage = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        " \n",
        "\t# return the image\n",
        "\treturn image\n",
        "\n",
        "n_of_training_images=150#the number of training images to use\n",
        "for progress in range(n_of_training_images):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not t_split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(t_split_urls[progress])\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/train/tusker/img'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "\n",
        "for progress in range(n_of_training_images):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not ae_split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(ae_split_urls[progress])\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/train/african/img'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "        \n",
        "for progress in range(n_of_training_images):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not ie_split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(ie_split_urls[progress])\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/train/indian/img'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "\n",
        "\n",
        "\n",
        "#Validation data:\n",
        "\n",
        "for progress in range(50):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not t_split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(t_split_urls[n_of_training_images+progress])#get images that are different from the ones used for training\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/validation/tusker/img'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "\n",
        "for progress in range(50):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not ae_split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(ae_split_urls[n_of_training_images+progress])#get images that are different from the ones used for training\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/validation/african/img'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "\n",
        "for progress in range(50):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not ie_split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(ie_split_urls[n_of_training_images+progress])#get images that are different from the ones used for training\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/validation/indian/img'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "\n",
        "\n",
        "print(\"\\nTRAIN:\\n\")          \n",
        "print(\"\\nlist the files inside tusker directory:\\n\")        \n",
        "!ls /content/train/tusker\n",
        "print(\"\\nlist the files inside african directory:\\n\")\n",
        "!ls /content/train/african\n",
        "print(\"\\nlist the files inside indian directory:\\n\")\n",
        "!ls /content/train/indian\n",
        "print(\"\\nVALIDATION:\\n\")\n",
        "print(\"\\nlist the files inside tusker directory:\\n\")        \n",
        "!ls /content/validation/tusker \n",
        "print(\"\\nlist the files inside african directory:\\n\")\n",
        "!ls /content/validation/african\n",
        "print(\"\\nlist the files inside indian directory:\\n\")\n",
        "!ls /content/validation/indian"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "120\n",
            "140\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "120\n",
            "140\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "120\n",
            "140\n",
            "0\n",
            "20\n",
            "40\n",
            "0\n",
            "20\n",
            "40\n",
            "0\n",
            "20\n",
            "40\n",
            "\n",
            "TRAIN:\n",
            "\n",
            "\n",
            "list the files inside tusker directory:\n",
            "\n",
            "img0.jpg    img124.jpg\timg141.jpg  img24.jpg  img45.jpg  img71.jpg  img87.jpg\n",
            "img100.jpg  img125.jpg\timg142.jpg  img27.jpg  img46.jpg  img73.jpg  img88.jpg\n",
            "img101.jpg  img126.jpg\timg143.jpg  img28.jpg  img49.jpg  img74.jpg  img89.jpg\n",
            "img102.jpg  img127.jpg\timg145.jpg  img29.jpg  img52.jpg  img75.jpg  img91.jpg\n",
            "img105.jpg  img128.jpg\timg148.jpg  img2.jpg   img53.jpg  img76.jpg  img92.jpg\n",
            "img109.jpg  img12.jpg\timg149.jpg  img32.jpg  img56.jpg  img77.jpg  img94.jpg\n",
            "img110.jpg  img130.jpg\timg14.jpg   img33.jpg  img57.jpg  img78.jpg  img95.jpg\n",
            "img112.jpg  img131.jpg\timg16.jpg   img34.jpg  img61.jpg  img79.jpg  img96.jpg\n",
            "img114.jpg  img132.jpg\timg17.jpg   img35.jpg  img62.jpg  img7.jpg   img97.jpg\n",
            "img115.jpg  img134.jpg\timg18.jpg   img36.jpg  img64.jpg  img81.jpg  img98.jpg\n",
            "img118.jpg  img136.jpg\timg19.jpg   img38.jpg  img65.jpg  img82.jpg  img99.jpg\n",
            "img11.jpg   img137.jpg\timg1.jpg    img39.jpg  img66.jpg  img83.jpg  img9.jpg\n",
            "img120.jpg  img139.jpg\timg21.jpg   img42.jpg  img69.jpg  img84.jpg\n",
            "img121.jpg  img13.jpg\timg23.jpg   img44.jpg  img6.jpg   img85.jpg\n",
            "\n",
            "list the files inside african directory:\n",
            "\n",
            "img0.jpg    img125.jpg\timg144.jpg  img27.jpg  img46.jpg  img67.jpg  img82.jpg\n",
            "img101.jpg  img126.jpg\timg145.jpg  img28.jpg  img47.jpg  img68.jpg  img85.jpg\n",
            "img105.jpg  img127.jpg\timg146.jpg  img2.jpg   img49.jpg  img69.jpg  img86.jpg\n",
            "img106.jpg  img129.jpg\timg147.jpg  img30.jpg  img4.jpg   img6.jpg   img87.jpg\n",
            "img107.jpg  img12.jpg\timg149.jpg  img31.jpg  img50.jpg  img70.jpg  img8.jpg\n",
            "img108.jpg  img130.jpg\timg14.jpg   img32.jpg  img52.jpg  img71.jpg  img90.jpg\n",
            "img109.jpg  img131.jpg\timg17.jpg   img34.jpg  img54.jpg  img72.jpg  img92.jpg\n",
            "img112.jpg  img132.jpg\timg19.jpg   img35.jpg  img55.jpg  img73.jpg  img93.jpg\n",
            "img113.jpg  img134.jpg\timg1.jpg    img36.jpg  img58.jpg  img75.jpg  img95.jpg\n",
            "img115.jpg  img135.jpg\timg20.jpg   img37.jpg  img59.jpg  img76.jpg  img96.jpg\n",
            "img116.jpg  img137.jpg\timg21.jpg   img39.jpg  img5.jpg   img77.jpg  img97.jpg\n",
            "img119.jpg  img138.jpg\timg22.jpg   img3.jpg   img61.jpg  img78.jpg  img98.jpg\n",
            "img120.jpg  img13.jpg\timg23.jpg   img40.jpg  img62.jpg  img79.jpg  img99.jpg\n",
            "img121.jpg  img140.jpg\timg24.jpg   img41.jpg  img63.jpg  img7.jpg   img9.jpg\n",
            "img122.jpg  img142.jpg\timg25.jpg   img43.jpg  img64.jpg  img80.jpg\n",
            "img123.jpg  img143.jpg\timg26.jpg   img45.jpg  img65.jpg  img81.jpg\n",
            "\n",
            "list the files inside indian directory:\n",
            "\n",
            "img0.jpg    img115.jpg\timg131.jpg  img22.jpg  img47.jpg  img65.jpg  img83.jpg\n",
            "img100.jpg  img117.jpg\timg133.jpg  img25.jpg  img48.jpg  img70.jpg  img87.jpg\n",
            "img103.jpg  img119.jpg\timg139.jpg  img2.jpg   img51.jpg  img71.jpg  img91.jpg\n",
            "img104.jpg  img11.jpg\timg145.jpg  img30.jpg  img52.jpg  img74.jpg  img92.jpg\n",
            "img105.jpg  img121.jpg\timg146.jpg  img31.jpg  img54.jpg  img76.jpg  img94.jpg\n",
            "img106.jpg  img122.jpg\timg148.jpg  img35.jpg  img55.jpg  img77.jpg  img95.jpg\n",
            "img107.jpg  img123.jpg\timg15.jpg   img3.jpg   img57.jpg  img78.jpg  img97.jpg\n",
            "img108.jpg  img125.jpg\timg17.jpg   img41.jpg  img5.jpg   img79.jpg  img98.jpg\n",
            "img110.jpg  img127.jpg\timg18.jpg   img44.jpg  img60.jpg  img7.jpg   img9.jpg\n",
            "img112.jpg  img12.jpg\timg1.jpg    img45.jpg  img63.jpg  img80.jpg\n",
            "img114.jpg  img130.jpg\timg20.jpg   img46.jpg  img64.jpg  img82.jpg\n",
            "\n",
            "VALIDATION:\n",
            "\n",
            "\n",
            "list the files inside tusker directory:\n",
            "\n",
            "img10.jpg  img17.jpg  img23.jpg  img2.jpg   img38.jpg  img45.jpg  img6.jpg\n",
            "img11.jpg  img18.jpg  img24.jpg  img31.jpg  img39.jpg  img46.jpg  img8.jpg\n",
            "img12.jpg  img19.jpg  img26.jpg  img32.jpg  img40.jpg  img47.jpg  img9.jpg\n",
            "img14.jpg  img20.jpg  img27.jpg  img34.jpg  img41.jpg  img49.jpg\n",
            "img15.jpg  img21.jpg  img28.jpg  img36.jpg  img42.jpg  img4.jpg\n",
            "img16.jpg  img22.jpg  img29.jpg  img37.jpg  img43.jpg  img5.jpg\n",
            "\n",
            "list the files inside african directory:\n",
            "\n",
            "img0.jpg   img16.jpg  img24.jpg  img30.jpg  img39.jpg  img46.jpg  img9.jpg\n",
            "img10.jpg  img17.jpg  img25.jpg  img31.jpg  img3.jpg   img48.jpg\n",
            "img11.jpg  img19.jpg  img26.jpg  img33.jpg  img41.jpg  img49.jpg\n",
            "img12.jpg  img1.jpg   img27.jpg  img36.jpg  img43.jpg  img4.jpg\n",
            "img13.jpg  img21.jpg  img28.jpg  img37.jpg  img44.jpg  img5.jpg\n",
            "img15.jpg  img23.jpg  img29.jpg  img38.jpg  img45.jpg  img6.jpg\n",
            "\n",
            "list the files inside indian directory:\n",
            "\n",
            "img0.jpg   img15.jpg  img24.jpg  img2.jpg   img42.jpg  img5.jpg\n",
            "img10.jpg  img18.jpg  img25.jpg  img31.jpg  img44.jpg  img6.jpg\n",
            "img11.jpg  img19.jpg  img27.jpg  img34.jpg  img46.jpg  img7.jpg\n",
            "img12.jpg  img20.jpg  img28.jpg  img38.jpg  img47.jpg  img9.jpg\n",
            "img13.jpg  img22.jpg  img29.jpg  img3.jpg   img4.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM40BlVhuqQF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "914808ce-3936-4b31-f49c-5bb1121bb05f"
      },
      "source": [
        "for progress in range(50):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not t_split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(t_split_urls[n_of_training_images+progress+50])#get images that are different from the ones used for training\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/test/test/tusker_'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "\n",
        "for progress in range(50):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not ae_split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(ae_split_urls[n_of_training_images+progress+50])#get images that are different from the ones used for training\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/test/test/african_'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None\n",
        "\n",
        "for progress in range(50):#store all the images on a directory\n",
        "    # Print out progress whenever progress is a multiple of 20 so we can follow the\n",
        "    # (relatively slow) progress\n",
        "    if(progress%20==0):\n",
        "        print(progress)\n",
        "    if not ie_split_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(ie_split_urls[n_of_training_images+progress+50])#get images that are different from the ones used for training\n",
        "        if (len(I.shape))==3: #check if the image has width, length and channels\n",
        "          save_path = '/content/test/test/indian_'+str(progress)+'.jpg'#create a name of each image\n",
        "          cv2.imwrite(save_path,I)\n",
        "\n",
        "      except:\n",
        "        None"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "20\n",
            "40\n",
            "0\n",
            "20\n",
            "40\n",
            "0\n",
            "20\n",
            "40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSlNCKJMGq7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path to the train folder\n",
        "original_test = '/content/test/test'\n",
        " \n",
        "filenames = os.listdir(original_test)\n",
        "categories = []\n",
        "for filename in filenames:\n",
        "    category = filename.split('_')[0]\n",
        "    if category == 'tusker':\n",
        "        categories.append('tusker')\n",
        "    elif category == 'african':\n",
        "        categories.append('african')\n",
        "    else:\n",
        "        categories.append('indian')\n",
        "\n",
        "data_test = pd.DataFrame({'filename':filenames,'label':categories})\n",
        "\n",
        "data_test.to_csv(\"original_test.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDQRgQz_QQhF",
        "colab_type": "text"
      },
      "source": [
        "#Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zEuli2HL8Pj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "88e848f2-acd4-4071-f325-d4b09b1d4c36"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=r\"/content/train/\",\n",
        "    target_size=(32, 32),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    class_mode=\"binary\",\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "valid_generator = test_datagen.flow_from_directory(\n",
        "    directory=r\"/content/validation/\",\n",
        "    target_size=(32, 32),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    class_mode=\"binary\",\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=\"/content/test/\",\n",
        "    target_size=(32, 32),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 281 images belonging to 3 classes.\n",
            "Found 105 images belonging to 3 classes.\n",
            "Found 95 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD-cSPUPM8Hy",
        "colab_type": "text"
      },
      "source": [
        "#Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypt-5dZnM7hT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "38bcfd0e-6c8f-4e53-94ae-1aa052fc9d44"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import models\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Flatten, Conv2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "#We need to use a Conv2D layer at start of the neural network \n",
        "#the syntax is Conv2D(1, (image_width,image_height), padding=\"valid\", activation=\"relu\", input_shape=X_train.shape[1:])\n",
        "#the we add a flatten layer\n",
        "model.add(Conv2D(512, (32, 32), padding=\"valid\", activation=\"relu\", input_shape=(32, 32, 3)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit_generator(generator=train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=valid_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID,\n",
        "                    epochs=10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-17f2e4e1e179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_VALID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                     epochs=10)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    814\u001b[0m                 \u001b[0;31m# using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                 check_loss_and_target_compatibility(\n\u001b[0;32m--> 816\u001b[0;31m                     y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    274\u001b[0m                 raise ValueError(\n\u001b[1;32m    275\u001b[0m                     \u001b[0;34m'You are passing a target array of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                     \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                     \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (32, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxslFZBwNQEo",
        "colab_type": "text"
      },
      "source": [
        "#Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URNPeVz1NRJB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "088bbcb0-a430-4dc9-d4bd-072fc76aa505"
      },
      "source": [
        "model.evaluate_generator(generator=valid_generator, steps=STEP_SIZE_VALID)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan, 0.2708333333333333]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crqtzRl9X2lZ",
        "colab_type": "text"
      },
      "source": [
        "#Measure the performance on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfKjajYHX5Q2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "c4adb651-d7c6-4399-b3f9-59bbdeba7029"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# AUC for prediction on validation sample\n",
        "X_val_sample, val_labels = next(valid_generator)\n",
        "val_pred = model.predict_proba(X_val_sample)\n",
        "val_pred = np.reshape(val_pred, val_labels.shape)\n",
        "val_score_auc = roc_auc_score(val_labels, val_pred)\n",
        "print (\"AUC validation score\")\n",
        "print (val_score_auc)\n",
        "print ('\\n')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-6354aa17dfeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mval_score_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC validation score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_score_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    379\u001b[0m                              \"instead\".format(max_fpr))\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         return _multiclass_roc_auc_score(y_true, y_score, labels,\n\u001b[1;32m    383\u001b[0m                                          multi_class, average, sample_weight)\n",
            "\u001b[0;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqoCLUOfNkom",
        "colab_type": "text"
      },
      "source": [
        "#Predict the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fxju6FFNmb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "test_generator.reset()\n",
        "pred=model.predict_generator(test_generator,\n",
        "steps=STEP_SIZE_TEST,\n",
        "verbose=1)\n",
        "\n",
        "\n",
        "predicted_class_indices = []\n",
        "for i in pred:\n",
        "    if i >=0.5:\n",
        "        predicted_class_indices.append(1)\n",
        "    else:\n",
        "        predicted_class_indices.append(0)\n",
        "\n",
        "\n",
        "labels = (train_generator.class_indices)\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "predictions = [labels[k] for k in predicted_class_indices]\n",
        "\n",
        "filenames=test_generator.filenames\n",
        "results=pd.DataFrame({\"Filename\":filenames,\n",
        "                      \"Predictions\":predictions})\n",
        "results.to_csv(\"results.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncYBMddc-Qxq",
        "colab_type": "text"
      },
      "source": [
        "#Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70xW8TGA-Sns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm=confusion_matrix(data_test['label'],results['Predictions'])\n",
        "\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFVzBJEj5sBO",
        "colab_type": "text"
      },
      "source": [
        "#Thank you for completing the notebook"
      ]
    }
  ]
}